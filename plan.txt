# 1) Tighten the problem statement

* Fix the kernel: focus on square SGEMM/DGEMM (float/double) `C := αAB + βC`.
* Define hardware targets (CPU model, core count/threads, cache sizes; GPU model/SMs; interconnect if cluster).
* Fix data types & layouts: row-major vs column-major; SoA/ AoS isn’t relevant here, but **B-transpose** choice is.

# 2) Baseline & methodology (repeatable)

* Warmups and median of N runs; pin CPU frequency (disable turbo if needed), isolate cores (taskset), set `OMP_PROC_BIND=close`.
* Report: problem size `n`, FLOP count (\~2n³), time, **GFLOP/s**, **Arithmetic Intensity (AI)** = FLOPs / bytes moved.
* Tools: `perf stat`, `likwid`, VTune/Amplifier (or `perf` counters), Nsight Compute for GPU, `nvidia-smi` for power, RAPL for CPU power.
* Plots: (i) Time vs n, (ii) GFLOP/s vs n, (iii) Roofline (GFLOP/s vs AI), (iv) Speedup vs threads, (v) Energy/solution.

---

# 3) Low-level C optimization (single core)

Target: get within \~60–80% of your CPU’s peak DGEMM with hand-tuned code.

**3.1 Algorithmic layout**

* **Loop order**: prefer `i-k-j` with **B pretransposed** so inner loop is unit-stride on both A and Bᵀ.
* **Blocking** (cache tiling): 3-level tiles `(Mc,Nc,Kc)` to fit L2/L3; micro-kernel `(Mr,Nr)` to registers.

  * Start with: `Kc` fits L2, `Mc` fits L2/L1, `Nr` small. Tune empirically.
* **Micro-kernel**: register-blocked inner kernel that computes an `Mr×Nr` C tile.

**3.2 SIMD & alignment**

* Use compiler auto-vectorization **plus** intrinsics:

  * AVX2: `_mm256_fmadd_ps/pd`; AVX-512: `_mm512_fmadd_ps/pd`, masks.
* Align allocations: `posix_memalign`/`aligned_alloc` (64-byte for AVX-512).
* Tell the compiler: `restrict`, `const`, `__builtin_assume_aligned(ptr, 64)`.
* Manual prefetch: `__builtin_prefetch(&A[i+pf], 0, 3)`; measure—remove if it hurts.

**3.3 Compiler flags**

* Start: `-O3 -march=native -ffp-contract=fast -funroll-loops`.
* Consider (with caveats): `-Ofast -fno-math-errno -ffast-math` (document numerical tradeoffs).
* Check `-fopenmp-simd` and `#pragma omp simd` on the innermost loop.

**3.4 What to report**

* Roofline position (compute-bound vs bandwidth-bound).
* Vectorization reports (`-fopt-info-vec`), IPC, L1/L2 miss rates.
* Compare: naïve triple loop vs cache-blocked vs vectorized micro-kernel vs **OpenBLAS/BLIS/MKL**.

---

# 4) Multicore with OpenMP (single node)

Goal: near-linear scale until you saturate memory bandwidth or reach peak compute.

**4.1 Parallelization**

* Tile outer loops; `#pragma omp parallel for collapse(2) schedule(static)` over `(i,j)` tiles.
* Keep per-thread working sets in cache; avoid false sharing (pad C tiles or use distinct ranges).

**4.2 Extra levers**

* `#pragma omp simd` inside the micro-kernel.
* Thread affinity: `OMP_PROC_BIND=close`, `OMP_PLACES=cores`.
* NUMA: if dual-socket, first-touch initialize C; or use `numactl --cpunodebind=… --membind=…`.

**4.3 Plots**

* Speedup vs threads, efficiency, and per-thread bandwidth from `perf`.

---

# 5) CUDA (single GPU)

Goal: show the classic progression to near-cuBLAS performance.

**5.1 Kernels to implement**

1. **Naïve** global-memory kernel (one thread = one C\[i,j]) — baseline.
2. **Tiled shared-memory** kernel (e.g., 32×32 tiles), loop over K tiles; unroll inner loop.
3. **Vectorized loads** (`float4`/`double2`), avoid bank conflicts, use `__ldg` as appropriate.
4. **Tensor Cores (WMMA)** path for FP16/BF16→FP32 accumulate (if hardware supports).

**5.2 Launch config & tuning**

* Block size 128–256 threads; 32×8 or 16×16; tune for occupancy vs register pressure.
* Check occupancy, global load efficiency, shared-mem throughput in Nsight Compute.

**5.3 Baselines**

* **cuBLAS** `cublasSgemm/Dgemm` as the “gold standard.”
* Plot: your kernels vs cuBLAS GFLOP/s; discuss memory-bound vs compute-bound via roofline.

---

# 6) MPI (multi-node)

Goal: strong & weak scaling with a 2D process grid.

**6.1 Decomposition**

* **Block-cyclic 2D** distribution of C, conforming A and B panels.
* Implement **SUMMA** (simpler than Cannon’s; great baseline): broadcast A panels across process rows, B panels across columns, compute local GEMMs.

**6.2 Overlap & collectives**

* Use non-blocking `MPI_Ibcast`/`MPI_Isend/Irecv` to overlap comms with compute.
* Consider topology-aware communicators; measure latency/bandwidth with OSU micro-benchmarks (optional).

**6.3 What to show**

* Strong scaling: fixed n, increase ranks → time & efficiency.
* Weak scaling: grow n with ranks → time flatness.
* Breakdowns: comms vs compute time; message sizes; network BW.

*(If you don’t have a cluster: emulate with multiple nodes on a cloud small cluster for a few runs.)*

---

# 7) Algorithmic variants & precision

* **Strassen/Winograd**: implement for larger n; discuss numerical stability trade-offs and cross-over points (expect benefit only at big sizes).
* **Mixed precision**: FP16/BF16 on Tensor Cores with iterative refinement to FP32/64 accuracy; show speedup vs error norms.
* **Sparsity** (if relevant): structured sparsity kernels, but that’s a different study—optional.

---

# 8) Validation & numerics

* Verify with reference NumPy/BLAS: compute max/relative error, Frobenius norm of residual.
* For mixed-precision/Tensor Cores, include iterative refinement steps and resulting error plots.

---

# 9) Energy & cost of solution

* CPU: Intel/AMD RAPL for package energy; GPU: `nvidia-smi --query-gpu=power.draw`.
* Report **Joules per 10⁹ FLOPs** and **GFLOP/s/W**; add “performance per dollar” if you have list prices.

---

# 10) Results you should expect (sanity checks)

* **C (naïve)**: tiny GFLOP/s; **blocked+SIMD**: big jump, often 10–30×.
* **OpenMP**: near-linear until memory BW wall; DGEMM should remain compute-bound if your micro-kernel is good.
* **BLAS (MKL/OpenBLAS/BLIS)**: hard to beat; use as upper bound.
* **CUDA tiled**: 60–80% of cuBLAS if well-tuned; **WMMA** can exceed FP32 cuBLAS if mixed precision acceptable.
* **MPI SUMMA**: good weak scaling; strong scaling limited by network and A/B broadcasts.

---

# 11) Paper structure (drop-in sections)

1. **Background & Roofline Model**
2. **Experimental Setup** (hardware, compilers, drivers, BLAS versions)
3. **CPU Single-Core Optimizations** (blocking, SIMD, micro-kernel)
4. **Multicore (OpenMP)** (tiling strategy, affinity)
5. **GPU (CUDA)** (kernels, shared memory, Tensor Cores, cuBLAS)
6. **Distributed (MPI/SUMMA)** (data dist., overlap, scaling)
7. **Algorithmic & Precision Variants** (Strassen, mixed precision)
8. **Numerical Accuracy** (error analysis)
9. **Energy & Cost** (GFLOP/s/W)
10. **Results & Discussion** (with roofline & scaling plots)
11. **Reproducibility Appendix** (build flags, commit hashes, scripts)

---

# 12) Quick start checklists

**CPU (GCC/Clang)**

* Build: `-O3 -march=native -ffp-contract=fast -funroll-loops` (+ `-Ofast` if allowed)
* Code: block 3-levels; micro-kernel with AVX2/AVX-512 intrinsics; `restrict` + aligned alloc.
* Compare: MKL / OpenBLAS / BLIS.

**OpenMP**

* `#pragma omp parallel for collapse(2) schedule(static)`
* `#pragma omp simd` on inner loop
* Env: `OMP_NUM_THREADS`, `OMP_PROC_BIND=close`, `OMP_PLACES=cores`

**CUDA**

* Kernels: naïve → shared-mem tiled (32×32) → vectorized → WMMA
* Metrics: occupancy, dram throughput, shared-mem efficiency
* Baseline: cuBLAS

**MPI**

* 2D process grid; SUMMA; non-blocking broadcasts; overlap compute
* Collect: strong/weak scaling

---


